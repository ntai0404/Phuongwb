# ğŸ“š KIáº¾N TRÃšC Há»† THá»NG PHUONG WEB

## ğŸ“‹ Má»¥c Lá»¥c
1. [Tá»•ng Quan Kiáº¿n TrÃºc](#tá»•ng-quan-kiáº¿n-trÃºc)
2. [CÃ¡c Microservices](#cÃ¡c-microservices)
3. [Luá»“ng Dá»¯ Liá»‡u](#luá»“ng-dá»¯-liá»‡u)
4. [Cáº¥u TrÃºc ThÆ° Má»¥c](#cáº¥u-trÃºc-thÆ°-má»¥c)
5. [Giáº£i ThÃ­ch Chi Tiáº¿t Tá»«ng File](#giáº£i-thÃ­ch-chi-tiáº¿t-tá»«ng-file)
6. [Quy TrÃ¬nh Hoáº¡t Äá»™ng](#quy-trÃ¬nh-hoáº¡t-Ä‘á»™ng)

---

## ğŸ¯ Tá»•ng Quan Kiáº¿n TrÃºc

**Phuong Web** lÃ  má»™t há»‡ thá»‘ng **tin tá»©c thÃ´ng minh** cÃ³ 3 cÃ¡ch láº¥y dá»¯ liá»‡u:

### 1. **RSS Feeds** (CÃ¡ch 1)
- Láº¥y tá»« RSS feed cá»§a cÃ¡c website tin tá»©c
- **Nhanh**, dá»¯ liá»‡u **cÃ³ sáºµn Ä‘á»‹nh dáº¡ng**
- ÄÆ°á»£c sá»­ dá»¥ng: VNExpress, DanTri, ThanhNien, TuoiTre

### 2. **Web Crawling** (CÃ¡ch 2 - Má»šI)
- Táº£i **HTML trá»±c tiáº¿p** tá»« website
- **Parse HTML** Ä‘á»ƒ trÃ­ch xuáº¥t **toÃ n bá»™ ná»™i dung**
- LÃ  **real crawling** thá»±c sá»± (giÃ¡o viÃªn sáº½ hÃ i lÃ²ng!)

### 3. **AI Classification** (Tá»± Ä‘á»™ng phÃ¢n loáº¡i)
- PhÃ¢n loáº¡i bÃ i viáº¿t vÃ o **8 danh má»¥c**
- Sá»­ dá»¥ng **Semantic embeddings** + **keyword matching**

---

## ğŸ”§ CÃ¡c Microservices

### 1. **Frontend Service** (`frontend/`)
**CÃ´ng Nghá»‡**: Next.js 14, React, TypeScript, Tailwind CSS

**Chá»©c NÄƒng**:
- Giao diá»‡n ngÆ°á»i dÃ¹ng (web app)
- Hiá»ƒn thá»‹ danh sÃ¡ch bÃ i viáº¿t
- Xem chi tiáº¿t bÃ i viáº¿t
- LÆ°u bÃ i viáº¿t yÃªu thÃ­ch
- Xem lá»‹ch sá»­ Ä‘á»c bÃ i

**Port**: 3000

---

### 2. **Core API Service** (`core-api-service/`)
**CÃ´ng Nghá»‡**: FastAPI, Python 3.11

**Chá»©c NÄƒng**:
- API chÃ­nh cá»§a há»‡ thá»‘ng
- Quáº£n lÃ½ users, articles, sources
- XÃ¡c thá»±c (authentication)
- LÆ°u bÃ i viáº¿t yÃªu thÃ­ch
- PhÃ¢n loáº¡i bÃ i viáº¿t tá»± Ä‘á»™ng

**Port**: 8080

**CÃ¡c Module**:
- `auth/`: Xá»­ lÃ½ Ä‘Äƒng nháº­p, Ä‘Äƒng kÃ½
- `articles/`: Quáº£n lÃ½ bÃ i viáº¿t
- `sources/`: Quáº£n lÃ½ RSS sources
- `crawler/`: KÃ­ch hoáº¡t crawling
- `classifier.py`: AI phÃ¢n loáº¡i bÃ i viáº¿t

---

### 3. **Crawler Service** (`crawler-service/`)
**CÃ´ng Nghá»‡**: Python 3.11, BeautifulSoup, Requests

**Chá»©c NÄƒng**:
- Crawl tá»« RSS feeds
- Crawl tá»« HTML website (real crawling)
- Gá»­i dá»¯ liá»‡u qua RabbitMQ message queue

**Port**: 8003

**CÃ¡c Module**:
- `main.py`: Khá»Ÿi Ä‘á»™ng service, RSS crawling
- `web_crawler.py`: **Web crawler chÃ­nh** - crawl HTML tá»« website

---

### 4. **Summary Service** (`summary-service/`)
**CÃ´ng Nghá»‡**: Python 3.11, Hugging Face Transformers

**Chá»©c NÄƒng**:
- TÃ³m táº¯t AI bÃ i viáº¿t dÃ i
- Sá»­ dá»¥ng mÃ´ hÃ¬nh BART

**Port**: 8004

---

### 5. **Recommendation Service** (`recommendation-service/`)
**CÃ´ng Nghá»‡**: Python 3.11

**Chá»©c NÄƒng**:
- Gá»£i Ã½ bÃ i viáº¿t dá»±a trÃªn lá»‹ch sá»­ Ä‘á»c
- Collaborative filtering

**Port**: 8005

---

## ğŸ“Š Luá»“ng Dá»¯ Liá»‡u

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NgÆ°á»i DÃ¹ng Web                       â”‚
â”‚              (Browser / Frontend)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“ HTTP Request
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Frontend (Next.js)                      â”‚
â”‚    - Hiá»ƒn thá»‹ tin tá»©c                                    â”‚
â”‚    - Xá»­ lÃ½ login/register                                â”‚
â”‚    - LÆ°u bÃ i yÃªu thÃ­ch                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â†“ API Call (REST)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Core API Service (FastAPI)                  â”‚
â”‚    - /api/v1/articles (GET articles)                     â”‚
â”‚    - /api/v1/auth/login (ÄÄƒng nháº­p)                     â”‚
â”‚    - /api/v1/articles/save (LÆ°u bÃ i)                    â”‚
â”‚    - /api/v1/crawler/trigger (KÃ­ch hoáº¡t)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                        â”‚
           â†“ (Trigger)                              â†“ (Query)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Crawler Service â”‚                      â”‚   Database   â”‚
    â”‚ - RSS Crawling  â”‚                      â”‚  (PostgreSQL)â”‚
    â”‚ - Web Crawling  â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“ (Message Queue)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RabbitMQ Queue     â”‚
    â”‚ (crawled_data)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â†“ (Consume)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Core API Worker    â”‚
    â”‚ - LÆ°u articles      â”‚
    â”‚ - PhÃ¢n loáº¡i (AI)    â”‚
    â”‚ - TÃ³m táº¯t (AI)      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c

```
phuong-web/
â”œâ”€â”€ frontend/                          # ğŸ¨ Next.js Frontend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx                   # Trang chÃ­nh
â”‚   â”‚   â”œâ”€â”€ layout.tsx                 # Layout chung
â”‚   â”‚   â”œâ”€â”€ admin/page.tsx             # Admin panel
â”‚   â”‚   â”œâ”€â”€ articles/page.tsx          # Trang bÃ i viáº¿t
â”‚   â”‚   â”œâ”€â”€ saved/page.tsx             # BÃ i viáº¿t lÆ°u
â”‚   â”‚   â””â”€â”€ history/page.tsx           # Lá»‹ch sá»­ Ä‘á»c
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ news-card.tsx              # Card hiá»ƒn thá»‹ bÃ i viáº¿t
â”‚   â”‚   â”œâ”€â”€ news-grid.tsx              # Grid danh sÃ¡ch bÃ i
â”‚   â”‚   â”œâ”€â”€ news-detail-modal.tsx      # ğŸ”‘ Modal chi tiáº¿t bÃ i viáº¿t
â”‚   â”‚   â”œâ”€â”€ sidebar.tsx                # Sidebar danh má»¥c
â”‚   â”‚   â”œâ”€â”€ topbar.tsx                 # Top bar search
â”‚   â”‚   â””â”€â”€ providers/
â”‚   â”‚       â”œâ”€â”€ auth-provider.tsx      # Auth context
â”‚   â”‚       â””â”€â”€ query-provider.tsx     # React Query
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ use-articles.ts            # Hook fetch articles
â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”œâ”€â”€ api.ts                     # API client
â”‚   â”‚   â””â”€â”€ api-client.ts              # Axios client
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ core-api-service/                  # ğŸ”Œ FastAPI Backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.py                    # Khá»Ÿi Ä‘á»™ng API
â”‚   â”‚   â”œâ”€â”€ classifier.py              # ğŸ¤– AI phÃ¢n loáº¡i bÃ i
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â””â”€â”€ db.py                  # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py              # Login/Register API
â”‚   â”‚   â”‚   â””â”€â”€ utils.py               # JWT, Password
â”‚   â”‚   â”œâ”€â”€ articles/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py              # GET/POST articles API
â”‚   â”‚   â”‚   â””â”€â”€ db.py                  # Article DB queries
â”‚   â”‚   â”œâ”€â”€ sources/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py              # RSS sources API
â”‚   â”‚   â”‚   â””â”€â”€ db.py                  # Source DB queries
â”‚   â”‚   â”œâ”€â”€ crawler/
â”‚   â”‚   â”‚   â””â”€â”€ routes.py              # Trigger crawler API
â”‚   â”‚   â””â”€â”€ worker.py                  # ğŸ”„ Worker xá»­ lÃ½ messages
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ crawler-service/                   # ğŸ•·ï¸ Web Crawler
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.py                    # ğŸ”‘ RSS crawling + RabbitMQ
â”‚   â”‚   â””â”€â”€ web_crawler.py             # ğŸ”‘ Web crawling tá»« HTML
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ test_integration.py        # Integration tests
â”‚   â””â”€â”€ pyproject.toml
â”‚
â”œâ”€â”€ summary-service/                   # ğŸ“ AI Summary
â”‚   â””â”€â”€ main.py                        # TÃ³m táº¯t bÃ i viáº¿t
â”‚
â”œâ”€â”€ recommendation-service/            # ğŸ’¡ Recommendations
â”‚   â””â”€â”€ main.py                        # Gá»£i Ã½ bÃ i viáº¿t
â”‚
â”œâ”€â”€ docker-compose.yml                 # ğŸ³ Docker Compose
â”œâ”€â”€ pyproject.toml                     # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸ” Giáº£i ThÃ­ch Chi Tiáº¿t Tá»«ng File

### **Frontend Files**

#### `frontend/components/news-detail-modal.tsx` ğŸ”‘ **QUAN TRá»ŒNG**
```typescript
// File nÃ y hiá»ƒn thá»‹ chi tiáº¿t bÃ i viáº¿t khi user click vÃ o má»™t bÃ i

export default function NewsDetailModal({ article, isOpen, onClose }) {
  // 1. Láº¥y dá»¯ liá»‡u user tá»« Auth context
  const { user } = useAuth();
  
  // 2. Láº¥y bÃ i viáº¿t liÃªn quan
  const { data: relatedArticles } = useRelatedArticles(article?.id);
  
  // 3. Xá»­ lÃ½ lÆ°u bÃ i viáº¿t
  const saveArticleMutation = useMutation({
    mutationFn: async (articleId) => {
      // Gá»i API POST /api/v1/articles/save/{articleId}
      const response = await fetch(
        `${API_URL}/api/v1/articles/save/${articleId}?user_id=${user.id}`,
        { method: isSaved ? 'DELETE' : 'POST' }
      );
      return response.json();
    }
  });
  
  // 4. ÄÃ¡nh dáº¥u bÃ i Ä‘Ã£ Ä‘á»c
  useEffect(() => {
    if (isOpen && article?.id) {
      // Gá»i API POST /api/v1/articles/read/{articleId}
      fetch(`${API_URL}/api/v1/articles/read/${article.id}?user_id=${user.id}`, {
        method: 'POST'
      });
    }
  }, [isOpen]);
  
  // 5. Hiá»ƒn thá»‹ HTML content tá»« crawler
  return (
    <div className="prose prose-lg">
      <h1>{article.title}</h1>
      <img src={article.image_url} />
      {/* ğŸ”‘ ÄÃ¢y lÃ  pháº§n quan trá»ng: render full HTML content */}
      <div dangerouslySetInnerHTML={{ __html: article.content }} />
    </div>
  );
}
```

**DÃ²ng cháº£y**:
1. User click vÃ o bÃ i viáº¿t â†’ Modal má»Ÿ
2. Gá»i API mark as read â†’ LÆ°u vÃ o DB
3. Hiá»ƒn thá»‹: Title â†’ Summary â†’ Image â†’ Full Content (tá»« crawler)
4. User click "LÆ°u" â†’ API POST /save â†’ DB lÆ°u

---

#### `frontend/app/page.tsx` (Trang chÃ­nh)
```typescript
// Hiá»ƒn thá»‹ danh sÃ¡ch bÃ i viáº¿t chÃ­nh

function HomeContent() {
  // 1. Láº¥y danh má»¥c Ä‘Æ°á»£c chá»n tá»« URL
  const selectedCategory = useSearchParams().get('category') || 'all';
  
  // 2. Fetch bÃ i viáº¿t tá»« API
  const { data: articles } = useArticles(); // GET /api/v1/articles
  
  // 3. Normalize category names tá»« API (business â†’ Kinh doanh)
  const normalizeCategoryName = (cat) => {
    const categoryMap = {
      'business': 'Kinh doanh',
      'technology': 'CÃ´ng nghá»‡',
      'sports': 'Thá»ƒ thao',
      'entertainment': 'Giáº£i trÃ­',
    };
    return categoryMap[cat] || cat;
  };
  
  // 4. Filter bÃ i theo danh má»¥c Ä‘Æ°á»£c chá»n
  const filtered = articles.filter(item => {
    const category = normalizeCategoryName(item.category);
    return selectedCategory === 'all' || category === selectedCategory;
  });
  
  return (
    <div>
      <Sidebar selectedCategory={selectedCategory} /> {/* Chá»n danh má»¥c */}
      <NewsGrid articles={filtered} /> {/* Hiá»ƒn thá»‹ danh sÃ¡ch */}
    </div>
  );
}
```

---

### **Backend Files**

#### `core-api-service/src/main.py` (Khá»Ÿi Ä‘á»™ng API)
```python
# FastAPI server chÃ­nh

from fastapi import FastAPI
from fastapi_cors import CORSMiddleware
from .models import init_db  # Khá»Ÿi táº¡o database
from .worker import start_background_worker  # Khá»Ÿi Ä‘á»™ng consumer RabbitMQ

app = FastAPI()

# ThÃªm CORS middleware Ä‘á»ƒ frontend gá»i Ä‘Æ°á»£c
app.add_middleware(CORSMiddleware, allow_origins=["*"])

# Khá»Ÿi táº¡o database
@app.on_event("startup")
def startup():
    init_db()  # Táº¡o tables náº¿u chÆ°a tá»“n táº¡i
    start_background_worker()  # Khá»Ÿi Ä‘á»™ng RabbitMQ consumer

# Gá»“m cÃ¡c routes
app.include_router(auth_router)      # /api/v1/auth (login, register)
app.include_router(articles_router)  # /api/v1/articles (GET, POST, save)
app.include_router(sources_router)   # /api/v1/sources (RSS sources)
app.include_router(crawler_router)   # /api/v1/crawler (trigger)
```

---

#### `core-api-service/src/articles/routes.py` (API bÃ i viáº¿t)
```python
# API endpoints cho bÃ i viáº¿t

@router.get("/api/v1/articles")
def get_articles(skip: int = 0, limit: int = 20):
    """
    Láº¥y danh sÃ¡ch bÃ i viáº¿t
    - GET: /api/v1/articles?skip=0&limit=20
    - Response: [{ id, title, summary, content, image_url, category, ... }]
    """
    articles = db.query(Article).offset(skip).limit(limit).all()
    return articles

@router.get("/api/v1/articles/{article_id}")
def get_article(article_id: int):
    """
    Láº¥y chi tiáº¿t 1 bÃ i viáº¿t
    - GET: /api/v1/articles/123
    - Response: { id: 123, title: "...", content: "...", ... }
    """
    article = db.query(Article).filter(Article.id == article_id).first()
    return article

@router.post("/api/v1/articles/save/{article_id}")
def save_article(article_id: int, user_id: int):
    """
    LÆ°u bÃ i viáº¿t yÃªu thÃ­ch
    - POST: /api/v1/articles/save/123?user_id=5
    - Xá»­ lÃ½: Táº¡o UserSavedArticle(user_id=5, article_id=123)
    """
    saved = db.query(UserSavedArticle).filter(
        UserSavedArticle.user_id == user_id,
        UserSavedArticle.article_id == article_id
    ).first()
    
    if not saved:
        db.add(UserSavedArticle(user_id=user_id, article_id=article_id))
        db.commit()
    
    return {"saved": True}

@router.post("/api/v1/articles/read/{article_id}")
def mark_as_read(article_id: int, user_id: int):
    """
    ÄÃ¡nh dáº¥u bÃ i Ä‘Ã£ Ä‘á»c (lÆ°u lá»‹ch sá»­)
    - POST: /api/v1/articles/read/123?user_id=5
    - Xá»­ lÃ½: Táº¡o UserReadingHistory
    """
    db.add(UserReadingHistory(
        user_id=user_id,
        article_id=article_id,
        read_at=datetime.now()
    ))
    db.commit()
    return {"read": True}
```

---

#### `core-api-service/src/worker.py` (Worker xá»­ lÃ½ messages)
```python
# Worker nháº­n messages tá»« RabbitMQ queue

import pika
import json
from .classifier import classify_article
from .models import Article, db

def start_background_worker():
    """
    Khá»Ÿi Ä‘á»™ng worker xá»­ lÃ½ messages
    
    Luá»“ng:
    1. Connect tá»›i RabbitMQ
    2. Subscribe vÃ o queue 'crawled_data'
    3. Má»—i khi cÃ³ message má»›i:
       - Parse JSON
       - PhÃ¢n loáº¡i bÃ i (AI)
       - TÃ³m táº¯t bÃ i (optional)
       - LÆ°u vÃ o database
    """
    connection = pika.BlockingConnection(pika.ConnectionParameters('rabbitmq'))
    channel = connection.channel()
    
    # Táº¡o queue náº¿u chÆ°a tá»“n táº¡i
    channel.queue_declare(queue='crawled_data', durable=True)
    
    def callback(ch, method, properties, body):
        # Nháº­n message tá»« crawler
        article_data = json.loads(body)
        
        # 1. PhÃ¢n loáº¡i
        category = classify_article(
            title=article_data['title'],
            summary=article_data['summary'],
            content=article_data['content']
        )
        
        # 2. Táº¡o Article object
        article = Article(
            title=article_data['title'],
            summary=article_data['summary'],
            content=article_data['content'],  # Full content tá»« crawler
            image_url=article_data.get('image_url'),
            link=article_data['link'],
            category=category,
            source_id=article_data['source_id']
        )
        
        # 3. LÆ°u vÃ o database
        db.add(article)
        db.commit()
        
        # 4. XÃ¡c nháº­n message Ä‘Ã£ xá»­ lÃ½
        ch.basic_ack(delivery_tag=method.delivery_tag)
    
    # Subscribe vÃ o queue
    channel.basic_consume(queue='crawled_data', on_message_callback=callback)
    channel.start_consuming()
```

---

#### `core-api-service/src/classifier.py` (AI phÃ¢n loáº¡i)
```python
# ğŸ¤– AI phÃ¢n loáº¡i bÃ i viáº¿t tá»± Ä‘á»™ng

from sentence_transformers import SentenceTransformer
import numpy as np

# Load mÃ´ hÃ¬nh multilingual embeddings má»™t láº§n
MODEL = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

# MÃ´ táº£ cÃ¡c danh má»¥c (tiáº¿ng Viá»‡t)
CATEGORY_DESCRIPTIONS = {
    "Kinh doanh": "Kinh táº¿, tÃ i chÃ­nh, doanh nghiá»‡p, chá»©ng khoÃ¡n",
    "CÃ´ng nghá»‡": "CÃ´ng nghá»‡, AI, pháº§n má»m, internet, Ä‘iá»‡n thoáº¡i",
    "Thá»ƒ thao": "Thá»ƒ thao, bÃ³ng Ä‘Ã¡, giáº£i Ä‘áº¥u, váº­n Ä‘á»™ng viÃªn",
    "Giáº£i trÃ­": "Giáº£i trÃ­, phim áº£nh, Ã¢m nháº¡c, nghá»‡ sÄ©",
    "ChÃ­nh trá»‹": "ChÃ­nh trá»‹, chÃ­nh phá»§, quá»‘c há»™i, chÃ­nh sÃ¡ch",
    "Sá»©c khá»e": "Sá»©c khá»e, bá»‡nh táº­t, y táº¿, thuá»‘c",
    "GiÃ¡o dá»¥c": "GiÃ¡o dá»¥c, trÆ°á»ng há»c, Ä‘áº¡i há»c, há»c sinh",
    "PhÃ¡p luáº­t": "PhÃ¡p luáº­t, luáº­t lá»‡, tÃ²a Ã¡n, quyá»n lá»£i",
}

def classify_article(title: str, summary: str, content: str) -> str:
    """
    PhÃ¢n loáº¡i bÃ i viáº¿t vÃ o 1 trong 8 danh má»¥c
    
    CÃ¡ch hoáº¡t Ä‘á»™ng:
    1. Káº¿t há»£p title + summary + content
    2. TÃ­nh embedding (vector 384 chiá»u)
    3. TÃ­nh similarity vá»›i embedding cá»§a má»—i danh má»¥c
    4. Return danh má»¥c cÃ³ similarity cao nháº¥t
    """
    # 1. Káº¿t há»£p text
    full_text = f"{title} {summary} {content}"[:512]  # Max 512 chars
    
    # 2. TÃ­nh embedding cá»§a bÃ i viáº¿t
    article_embedding = MODEL.encode(full_text, convert_to_tensor=True)
    
    # 3. TÃ­nh similarity vá»›i tá»«ng danh má»¥c
    max_similarity = -1
    best_category = "KhÃ¡c"
    
    for category, description in CATEGORY_DESCRIPTIONS.items():
        category_embedding = MODEL.encode(description, convert_to_tensor=True)
        similarity = MODEL.util.pytorch_cos_sim(article_embedding, category_embedding)
        
        if similarity > max_similarity:
            max_similarity = similarity
            best_category = category
    
    return best_category
```

---

### **Crawler Files** ğŸ•·ï¸

#### `crawler-service/src/main.py` (RSS Crawling)
```python
# Khá»Ÿi Ä‘á»™ng crawler service
# CÃ³ 2 chá»©c nÄƒng: RSS crawling + Web crawling

import feedparser
from web_crawler import ArticleCrawler

def fetch_feed(url: str, max_items: int = 50) -> list:
    """
    Crawl RSS feed tá»« website
    
    QuÃ¡ trÃ¬nh:
    1. Parse RSS feed báº±ng feedparser
    2. Vá»›i má»—i article trong feed:
       a. Láº¥y title, summary tá»« RSS
       b. Crawl FULL content tá»« website (real crawling) â† ğŸ”‘ Má»›i!
       c. Láº¥y image URL
       d. Return dict: {title, summary, content, image_url, link, ...}
    
    VÃ­ dá»¥:
    """
    parsed = feedparser.parse(url)
    articles = []
    
    web_crawler = ArticleCrawler()  # â† Khá»Ÿi Ä‘á»™ng web crawler
    
    for entry in parsed.entries[:max_items]:
        # Láº¥y tá»« RSS
        title = entry.title
        summary = entry.summary
        link = entry.link
        
        # ğŸ”‘ Crawl full content tá»« website HTML
        crawled_data = web_crawler.crawl_article(link)
        if crawled_data and crawled_data.get('success'):
            full_content = crawled_data.get('content', summary)
        else:
            full_content = summary  # Fallback to RSS summary
        
        # Láº¥y image
        image_url = extract_image_from_feed(entry)
        
        articles.append({
            'title': title,
            'summary': summary,
            'content': full_content,  # â† FULL content, not just summary!
            'link': link,
            'image_url': image_url,
        })
    
    return articles

def process_crawl_task(ch, method, properties, body):
    """
    RabbitMQ Consumer: Nháº­n task tá»« queue, crawl, gá»­i data vá» API
    
    Luá»“ng:
    1. Nháº­n message: {source_id, url}
    2. Gá»i fetch_feed(url) â†’ Crawl RSS + HTML
    3. Gá»­i tá»«ng article vÃ o queue 'crawled_data'
    4. Core API worker nháº­n â†’ PhÃ¢n loáº¡i â†’ LÆ°u DB
    """
    task = json.loads(body)
    url = task.get('url')
    source_id = task.get('source_id')
    
    # Crawl RSS (+ web crawling bÃªn trong)
    articles = fetch_feed(url, max_items=50)
    
    for article in articles:
        article['source_id'] = source_id
        
        # Gá»­i vÃ o queue crawled_data
        publish_crawled_data(article)
    
    ch.basic_ack(delivery_tag=method.delivery_tag)
```

---

#### `crawler-service/src/web_crawler.py` (Web Crawling) ğŸ”‘ **QUAN TRá»ŒNG**
```python
# ğŸ•·ï¸ Web Crawler: Crawl full HTML content tá»« website
# ÄÃ¢y lÃ  "real crawling" thá»±c sá»±!

from bs4 import BeautifulSoup
import requests
import re

class ArticleCrawler:
    def crawl_article(self, url: str) -> dict:
        """
        Crawl toÃ n bá»™ content tá»« website HTML
        
        QuÃ¡ trÃ¬nh:
        1. Download HTML tá»« URL
        2. Parse HTML báº±ng BeautifulSoup
        3. TÃ¬m container chá»©a article (VNExpress: <article class="fck_detail">)
        4. TrÃ­ch xuáº¥t: paragraphs + images theo thá»© tá»± gá»‘c
        5. Filter bá»: scripts, ads, JavaScript code, timestamps
        6. Return HTML clean
        """
        
        # 1. XÃ¡c Ä‘á»‹nh website vÃ  route tá»›i crawler phÃ¹ há»£p
        if 'vnexpress.net' in url:
            return self.crawl_vnexpress(url)
        elif 'dantri.com.vn' in url:
            return self.crawl_dantri(url)
        else:
            return self.crawl_generic(url)
    
    def crawl_vnexpress(self, url: str) -> dict:
        """
        Crawl VNExpress article
        
        HTML structure:
        <article class="fck_detail">
          <p class="Normal">Paragraph 1</p>
          <p class="Normal">Paragraph 2</p>
          <img src="..." alt="..." />
          <p class="Normal">Paragraph 3</p>
        </article>
        """
        # 1. Download HTML
        response = requests.get(url, headers=self.headers)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 2. TÃ¬m article container
        article_body = soup.find('article', class_='fck_detail')
        if not article_body:
            return None
        
        # 3. TrÃ­ch xuáº¥t content (xem hÃ m dÆ°á»›i)
        content_html = self._collect_paragraphs_with_images(
            article_body, 
            url, 
            p_selector={'class_': 'Normal'}
        )
        
        return {'content': content_html, 'success': True}
    
    def _collect_paragraphs_with_images(self, container, page_url, p_selector=None):
        """
        TrÃ­ch xuáº¥t paragraphs + images theo thá»© tá»± gá»‘c
        
        ğŸ”‘ Quan trá»ng: Filter bá» táº¥t cáº£ bad content:
        - JavaScript code
        - Ads (taboola, outbrain)
        - Timestamps (GMT, UTC)
        
        QuÃ¡ trÃ¬nh:
        1. XÃ³a <script>, <style>, <iframe> tags
        2. XÃ³a div cÃ³ class chá»©a 'ad', 'taboola', v.v.
        3. Duyá»‡t qua cÃ¡c child elements theo thá»© tá»±
        4. Vá»›i má»—i element:
           - Náº¿u <p>: trÃ­ch text, filter JS code, add vÃ o content
           - Náº¿u <img>: get URL, add vÃ o content
           - Náº¿u <figure>: get image + caption, add vÃ o content
        5. Return '\n'.join(content_parts)
        """
        
        # Remove unwanted elements first
        for unwanted in container.find_all(['script', 'style', 'iframe', 'noscript']):
            unwanted.decompose()
        
        # Remove ads
        for ad in container.find_all('div', class_=re.compile(r'ad|taboola|outbrain', re.I)):
            ad.decompose()
        
        content_parts = []
        
        # Process each child in order
        for elem in container.children:
            if isinstance(elem, str):
                text = str(elem).strip()
                if text and not self._is_javascript_content(text):
                    content_parts.append(f'<p>{text}</p>')
                continue
            
            if elem.name == 'p':
                text = elem.get_text(strip=True)
                if text and not self._is_javascript_content(text):
                    content_parts.append(f'<p>{text}</p>')
            
            elif elem.name == 'img':
                img_src = elem.get('data-src') or elem.get('src')
                abs_src = self._abs_url(img_src, page_url)
                if abs_src:
                    content_parts.append(f'<img src="{abs_src}" alt="" />')
            
            elif elem.name == 'figure':
                img = elem.find('img')
                if img:
                    img_src = img.get('src')
                    abs_src = self._abs_url(img_src, page_url)
                    if abs_src:
                        content_parts.append(f'<img src="{abs_src}" />')
                
                # Add caption if exists
                figcaption = elem.find('figcaption')
                if figcaption:
                    caption_text = figcaption.get_text(strip=True)
                    if caption_text:
                        content_parts.append(f'<p><em>{caption_text}</em></p>')
        
        return '\n'.join(content_parts)
    
    def _is_javascript_content(self, text: str) -> bool:
        """
        Filter JavaScript code, ads, timestamps
        
        Detect:
        - Keywords: 'taboola', 'window.', 'function(', etc.
        - Timestamps: '2024-01-10 12:30:00 GMT'
        """
        text_lower = text.lower()
        
        # Hard filter for ads
        ads_keywords = [
            'taboola', 'outbrain', 'arfasync', 'mutexads',
            'window.runinit', 'pageSettings', 'querySelector'
        ]
        
        for keyword in ads_keywords:
            if keyword in text_lower:
                return True
        
        # Detect timestamps
        if re.search(r'\d{4}-\d{2}-\d{2}.*GMT', text):
            return True
        
        # Count JS patterns
        js_patterns = ['var ', 'const ', 'function(', 'if (', 'for (']
        if sum(1 for p in js_patterns if p in text_lower) >= 3:
            return True
        
        return False
```

---

## ğŸ”„ Quy TrÃ¬nh Hoáº¡t Äá»™ng (End-to-End)

### **Scenario 1: User Xem BÃ i Viáº¿t**

```
1. User má»Ÿ web â†’ frontend/app/page.tsx
   â”œâ”€ Fetch articles: GET /api/v1/articles
   â””â”€ Display danh sÃ¡ch bÃ i viáº¿t

2. User click vÃ o 1 bÃ i â†’ news-detail-modal.tsx
   â”œâ”€ Modal má»Ÿ
   â”œâ”€ Gá»i API POST /api/v1/articles/read/{id}?user_id=X
   â”‚  â””â”€ Worker lÆ°u vÃ o UserReadingHistory
   â””â”€ Hiá»ƒn thá»‹:
      â”œâ”€ Title + Summary
      â”œâ”€ Image
      â””â”€ Full Content (tá»« crawler) â† ğŸ”‘ Web Crawling result!

3. User click "LÆ°u" â†’ news-detail-modal.tsx
   â”œâ”€ Gá»i API POST /api/v1/articles/save/{id}?user_id=X
   â””â”€ Worker lÆ°u vÃ o UserSavedArticle
```

### **Scenario 2: Admin KÃ­ch Hoáº¡t Crawl**

```
1. Admin vÃ o /admin â†’ admin/page.tsx
   â””â”€ Click "KÃ­ch hoáº¡t thu tháº­p"

2. Frontend gá»­i â†’ POST /api/v1/crawler/trigger
   â””â”€ Core API nháº­n

3. Core API â†’ gá»­i message vÃ o RabbitMQ queue: 'crawl_tasks'
   â””â”€ {source_id: 1, url: "https://vnexpress.net/rss/..."}

4. Crawler Service consumer nháº­n message
   â”œâ”€ Gá»i fetch_feed(url)
   â”‚  â”œâ”€ Parse RSS feed
   â”‚  â””â”€ Vá»›i má»—i article:
   â”‚     â”œâ”€ Láº¥y title, summary tá»« RSS
   â”‚     â”œâ”€ ğŸ•·ï¸ Crawl full content tá»« website (web_crawler.py)
   â”‚     â”œâ”€ Láº¥y image
   â”‚     â””â”€ Gá»­i vÃ o queue 'crawled_data'

5. Core API worker nháº­n tá»« 'crawled_data'
   â”œâ”€ PhÃ¢n loáº¡i bÃ i (AI) â†’ classifier.py
   â”œâ”€ TÃ³m táº¯t bÃ i (optional) â†’ summary service
   â””â”€ LÆ°u vÃ o database

6. Frontend fetch láº¡i â†’ GET /api/v1/articles
   â””â”€ Hiá»ƒn thá»‹ bÃ i viáº¿t má»›i cÃ¹ng full content
```

---

## ğŸ“Š Database Schema

```
Users
â”œâ”€ id (Primary Key)
â”œâ”€ username (Unique)
â”œâ”€ password_hash
â”œâ”€ email
â””â”€ role (user, admin)

Articles
â”œâ”€ id (Primary Key)
â”œâ”€ title
â”œâ”€ summary
â”œâ”€ content (ğŸ”‘ Full HTML content tá»« crawler)
â”œâ”€ link
â”œâ”€ image_url
â”œâ”€ category (Kinh doanh, CÃ´ng nghá»‡, ...)
â”œâ”€ source_id (Foreign Key â†’ RSSSource)
â”œâ”€ published_at
â””â”€ fetched_at

RSSSource
â”œâ”€ id (Primary Key)
â”œâ”€ name (VNExpress, DanTri, ...)
â”œâ”€ url (RSS feed URL)
â”œâ”€ category
â””â”€ is_active

UserSavedArticle
â”œâ”€ user_id (Foreign Key)
â”œâ”€ article_id (Foreign Key)
â””â”€ saved_at

UserReadingHistory
â”œâ”€ user_id (Foreign Key)
â”œâ”€ article_id (Foreign Key)
â””â”€ read_at
```

---

## ğŸš€ Deployment

```bash
# 1. Build all services
docker-compose build

# 2. Start all services
docker-compose up -d

# 3. Services running
- Frontend: http://localhost:3000
- API: http://localhost:8080
- Crawler: http://localhost:8003
- Summary: http://localhost:8004
- Recommendation: http://localhost:8005
- Database: localhost:5432
- Redis: localhost:6379
- RabbitMQ: localhost:5672
```

---

## ğŸ’¡ Key Technologies

| Service | Tech | Version |
|---------|------|---------|
| Frontend | Next.js + React + TypeScript | 14.0 |
| API | FastAPI + SQLAlchemy | 0.104 |
| Crawler | BeautifulSoup4 + Requests | 4.12 |
| Database | PostgreSQL | 15 |
| Queue | RabbitMQ | 3.12 |
| Cache | Redis | 7.0 |
| ML | Sentence Transformers | 2.2 |
| Summary | Hugging Face BART | - |

---

## ğŸ¯ TÃ³m Táº¯t

**Phuong Web** = Há»‡ thá»‘ng tin tá»©c thÃ´ng minh vá»›i:
- âœ… **RSS Crawling** (nhanh, dá»¯ liá»‡u Ä‘á»‹nh sáºµn)
- âœ… **Web Crawling** (real crawling tá»« HTML)
- âœ… **AI Classification** (phÃ¢n loáº¡i tá»± Ä‘á»™ng)
- âœ… **Microservices** (scalable, modular)
- âœ… **Full-featured** (save, history, recommendation)

**Kiáº¿n trÃºc**:
```
Frontend (Next.js) 
  â†” 
Core API (FastAPI)
  â†•
Crawler Service (Python) â†’ RabbitMQ â†’ Worker â†’ AI Classification
  â†•
PostgreSQL Database
```

---

**TÃ¡c giáº£**: AI Assistant  
**NgÃ y cáº­p nháº­t**: 11/01/2026  
**PhiÃªn báº£n**: 1.0
